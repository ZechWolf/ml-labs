{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Report\n",
    "\n",
    "Prepared by Zech Wolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business understanding\n",
    "\n",
    "write stuff here\n",
    "\n",
    "https://www.kaggle.com/datasets/whenamancodes/students-performance-in-exams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data preprocessing\n",
    "\n",
    "There are three steps to prepare this dataset for classification with logistic regression\n",
    "* Discretize math score as \"failing\", \"passing\", or \"exceptional\" (0,1,2)\n",
    "* One hot encode the categorical features\n",
    "* Standardize writing and reading test grades as z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   gender                       1000 non-null   object\n",
      " 1   race/ethnicity               1000 non-null   object\n",
      " 2   parental level of education  1000 non-null   object\n",
      " 3   lunch                        1000 non-null   object\n",
      " 4   test preparation course      1000 non-null   object\n",
      " 5   math score                   1000 non-null   int64 \n",
      " 6   reading score                1000 non-null   int64 \n",
      " 7   writing score                1000 non-null   int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in dataframe\n",
    "df = pd.read_csv(\"datasets/exams.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Encode parental education as an ordinal feature\n",
    "def encode_ed(edlevel: str):\n",
    "    ordering = {\"some high school\": 1, \"high school\": 2, \"some college\": 3,\n",
    "                \"associate's degree\": 4, \"bachelor's degree\": 5, \"master's degree\": 6}\n",
    "\n",
    "    return ordering[edlevel]\n",
    "\n",
    "encode_ed = np.vectorize(encode_ed)\n",
    "df[\"parent_ed\"] = encode_ed(df[\"parental level of education\"])\n",
    "\n",
    "#Create target variable from math score\n",
    "def encode_score(score: int):\n",
    "    if score < 70: #failing\n",
    "        return 0\n",
    "    elif score >= 70 and score < 85: #passing\n",
    "        return 1\n",
    "    else: #exceptional\n",
    "        return 2\n",
    "\n",
    "encode_score = np.vectorize(encode_score)\n",
    "df[\"math_score\"] = encode_score(df[\"math score\"])\n",
    "\n",
    "#One hot encoding for the remaining nominal features\n",
    "df = pd.get_dummies(data=df, columns=[\"gender\", \"race/ethnicity\", \"lunch\", \"test preparation course\"], drop_first=True)\n",
    "df.drop(columns=[\"parental level of education\"], inplace=True) #drop old education column\n",
    "\n",
    "#Normalize numeric columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df[[\"writing score\", \"reading score\"]] = StandardScaler().fit_transform(df[[\"writing score\", \"reading score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   math score                    1000 non-null   int64  \n",
      " 1   reading score                 1000 non-null   float64\n",
      " 2   writing score                 1000 non-null   float64\n",
      " 3   parent_ed                     1000 non-null   int64  \n",
      " 4   math_score                    1000 non-null   int64  \n",
      " 5   gender_male                   1000 non-null   uint8  \n",
      " 6   race/ethnicity_group B        1000 non-null   uint8  \n",
      " 7   race/ethnicity_group C        1000 non-null   uint8  \n",
      " 8   race/ethnicity_group D        1000 non-null   uint8  \n",
      " 9   race/ethnicity_group E        1000 non-null   uint8  \n",
      " 10  lunch_standard                1000 non-null   uint8  \n",
      " 11  test preparation course_none  1000 non-null   uint8  \n",
      "dtypes: float64(2), int64(3), uint8(7)\n",
      "memory usage: 46.0 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([566, 308, 126]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "np.unique(df[\"math_score\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zech/anaconda3/envs/mlenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Get X and y arrays from df\n",
    "X = df.drop(columns=[\"math_score\"]).to_numpy()\n",
    "y = df[\"math_score\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y) #80/20 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from example notebook\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegressionBasee): #changed this to extend the base class (non-vectorized version is not needed)\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_intercept=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    \"\"\"\n",
    "    This is the underlying class used for one-versus-all in the main logistic regression\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty=\"l2\", solver=\"sa\", C=1.0, eta=0.1, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.solver = solver\n",
    "        self.penalty = penalty\n",
    "        self.C = C #store all the hyperparameters for use later\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "\n",
    "    def _get_gradient(self, X, y):\n",
    "\n",
    "        ydiff = y-self.predict_proba(X,add_intercept=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "\n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", solver=\"sa\", C=1.0, eta=0.1, iterations=20):\n",
    "        \"\"\"\n",
    "        Construct a multi-class logistic regression classifier.\n",
    "        This class implements one-versus-all classification using BinaryLogisticRegression classifiers.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        penalty : regularization strategy as one of {\"none\", \"l1\", \"l2\", \"l1l2\"}\n",
    "        solver : optimization strategy as one of {\"sa\", \"sga\", \"newton\", \"bfgs\"}\n",
    "        C : regularization cost\n",
    "        eta : gradient step amount when updating weights\n",
    "        iterations : number of iterations before stopping\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.solver = solver\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = BinaryLogisticRegression(self.eta, self.iters, self.solver, self.penalty, self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row\n",
    "\n",
    "LogisticRegression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2b76c06a6a9823cc670680360b93737c31e051c9eb2b7e2fa79d5c16cf7eced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
