{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 Report\n",
    "\n",
    "Prepared by Zech Wolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is the handwritten letters dataset from EMNIST. There are 26 classes for each of the letters in the Roman alphabet. The full dataset contains over 145,000 instances. The provided training set is used as the full dataset for this report (about 88,000 instances). Each instance is a 28x28 grayscale image containing a handwritten letter.\n",
    "\n",
    "Dataset source: https://www.kaggle.com/datasets/crawford/emnist?resource=download&select=emnist-letters-train.csv (processed into .csv format from original source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Performance metrics\n",
    "\n",
    "A possible business case for this dataset is optical character recognition (OCR), which converts text from an image into digital text on a computer. A classification model trained on this dataset could be used to process handwriting.\n",
    "\n",
    "The proper metric is accuracy, since all classes are equally important, and the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cross validation method\n",
    "\n",
    "With nearly 90,000 instances, the dataset is very large. The dataset is also balanced by class. For these reasons, a stratified 80/20 split should be suitable for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/emnist-letters-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X = df.drop(columns=df.columns[0]).to_numpy(dtype=np.float32)\n",
    "X = X.reshape((len(X),28,28,1))  #reshape to tensor format\n",
    "\n",
    "y = df[df.columns[0]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1234)\n",
    "X_train = tf.image.per_image_standardization(X_train)\n",
    "X_test = tf.image.per_image_standardization(X_test) #normalization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data augmentation\n",
    "\n",
    "Random rotations and random brightness changes are applied to the dataset as preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddfd717d891ac6057412802ae0a4961222818e49fb4b44e9024a7e0c52b14e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
